{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "class DAGMDataset(Dataset):\n",
    "    def __init__(self, image_dir, label_dir, mapping_file, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.transform = transform\n",
    "\n",
    "        self.samples = []\n",
    "\n",
    "        with open(mapping_file, 'r') as file:\n",
    "            for line in file:\n",
    "                parts = line.strip().split()\n",
    "                if len(parts) < 5:\n",
    "                    continue\n",
    "                img_file = parts[2]\n",
    "                label_flag = int(parts[1])\n",
    "                label_file = parts[4] if label_flag == 1 else None\n",
    "                self.samples.append((img_file, label_file))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_file, label_file = self.samples[idx]\n",
    "\n",
    "        img_path = os.path.join(self.image_dir, img_file)\n",
    "        image = Image.open(img_path).convert('L').resize((256, 256))\n",
    "        image = np.array(image, dtype=np.float32) / 255.0\n",
    "        image = torch.tensor(image).unsqueeze(0)  # Shape: [1, H, W]\n",
    "\n",
    "        if label_file and label_file != \"0\":\n",
    "            label_path = os.path.join(self.label_dir, label_file)\n",
    "            mask = Image.open(label_path).convert('L').resize((256, 256))\n",
    "            mask = (np.array(mask, dtype=np.uint8) > 127).astype(np.float32)\n",
    "        else:\n",
    "            mask = np.zeros((256, 256), dtype=np.float32)\n",
    "\n",
    "        mask = torch.tensor(mask).unsqueeze(0)  # Shape: [1, H, W]\n",
    "\n",
    "        return image, mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape: torch.Size([1, 256, 256])\n",
      "Mask shape : torch.Size([1, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "image_dir = \"../static/DAGM_KaggleUpload/Class1/Train\"\n",
    "label_dir = \"../static/DAGM_KaggleUpload/Class1/Train/Label\"\n",
    "mapping_file = \"../static/DAGM_KaggleUpload/Class1/Train/Label/Labels.txt\"\n",
    "\n",
    "dataset = DAGMDataset(image_dir, label_dir, mapping_file)\n",
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "# Preview one sample\n",
    "img, mask = dataset[0]\n",
    "print(\"Image shape:\", img.shape)\n",
    "print(\"Mask shape :\", mask.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels=1, out_channels=1):\n",
    "        super(UNet, self).__init__()\n",
    "\n",
    "        def conv_block(in_ch, out_ch):\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
    "                nn.ReLU(inplace=True)\n",
    "            )\n",
    "\n",
    "        self.down1 = conv_block(in_channels, 64)\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "\n",
    "        self.down2 = conv_block(64, 128)\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "\n",
    "        self.down3 = conv_block(128, 256)\n",
    "        self.pool3 = nn.MaxPool2d(2)\n",
    "\n",
    "        self.down4 = conv_block(256, 512)\n",
    "        self.pool4 = nn.MaxPool2d(2)\n",
    "\n",
    "        self.bottleneck = conv_block(512, 1024)\n",
    "\n",
    "        self.up4 = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2)\n",
    "        self.dec4 = conv_block(1024, 512)\n",
    "\n",
    "        self.up3 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
    "        self.dec3 = conv_block(512, 256)\n",
    "\n",
    "        self.up2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        self.dec2 = conv_block(256, 128)\n",
    "\n",
    "        self.up1 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.dec1 = conv_block(128, 64)\n",
    "\n",
    "        self.final = nn.Conv2d(64, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        d1 = self.down1(x)\n",
    "        p1 = self.pool1(d1)\n",
    "\n",
    "        d2 = self.down2(p1)\n",
    "        p2 = self.pool2(d2)\n",
    "\n",
    "        d3 = self.down3(p2)\n",
    "        p3 = self.pool3(d3)\n",
    "\n",
    "        d4 = self.down4(p3)\n",
    "        p4 = self.pool4(d4)\n",
    "\n",
    "        bn = self.bottleneck(p4)\n",
    "\n",
    "        up4 = self.up4(bn)\n",
    "        merge4 = torch.cat([up4, d4], dim=1)\n",
    "        dec4 = self.dec4(merge4)\n",
    "\n",
    "        up3 = self.up3(dec4)\n",
    "        merge3 = torch.cat([up3, d3], dim=1)\n",
    "        dec3 = self.dec3(merge3)\n",
    "\n",
    "        up2 = self.up2(dec3)\n",
    "        merge2 = torch.cat([up2, d2], dim=1)\n",
    "        dec2 = self.dec2(merge2)\n",
    "\n",
    "        up1 = self.up1(dec2)\n",
    "        merge1 = torch.cat([up1, d1], dim=1)\n",
    "        dec1 = self.dec1(merge1)\n",
    "\n",
    "        return torch.sigmoid(self.final(dec1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model initialized on: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = UNet(in_channels=1, out_channels=1).to(device)\n",
    "print(\"Model initialized on:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.BCELoss()  # For binary segmentation\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split, DataLoader\n",
    "# 1. Prepare Train & Validation Loaders\n",
    "\n",
    "# Split dataset 80% train, 20% val\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/50] ğŸ‹ï¸ Train Loss: 0.3119 | ğŸ§ª Val Loss: 0.0375\n",
      "[2/50] ğŸ‹ï¸ Train Loss: 0.0410 | ğŸ§ª Val Loss: 0.0328\n",
      "[3/50] ğŸ‹ï¸ Train Loss: 0.0358 | ğŸ§ª Val Loss: 0.0288\n",
      "[4/50] ğŸ‹ï¸ Train Loss: 0.0346 | ğŸ§ª Val Loss: 0.0273\n",
      "[5/50] ğŸ‹ï¸ Train Loss: 0.0320 | ğŸ§ª Val Loss: 0.0278\n",
      "[6/50] ğŸ‹ï¸ Train Loss: 0.0327 | ğŸ§ª Val Loss: 0.0264\n",
      "[7/50] ğŸ‹ï¸ Train Loss: 0.0308 | ğŸ§ª Val Loss: 0.0260\n",
      "[8/50] ğŸ‹ï¸ Train Loss: 0.0309 | ğŸ§ª Val Loss: 0.0344\n",
      "[9/50] ğŸ‹ï¸ Train Loss: 0.0308 | ğŸ§ª Val Loss: 0.0302\n",
      "[10/50] ğŸ‹ï¸ Train Loss: 0.0292 | ğŸ§ª Val Loss: 0.0253\n",
      "[11/50] ğŸ‹ï¸ Train Loss: 0.0298 | ğŸ§ª Val Loss: 0.0260\n",
      "[12/50] ğŸ‹ï¸ Train Loss: 0.0302 | ğŸ§ª Val Loss: 0.0266\n",
      "[13/50] ğŸ‹ï¸ Train Loss: 0.0293 | ğŸ§ª Val Loss: 0.0249\n",
      "[14/50] ğŸ‹ï¸ Train Loss: 0.0284 | ğŸ§ª Val Loss: 0.0288\n",
      "[15/50] ğŸ‹ï¸ Train Loss: 0.0286 | ğŸ§ª Val Loss: 0.0242\n",
      "[16/50] ğŸ‹ï¸ Train Loss: 0.0285 | ğŸ§ª Val Loss: 0.0239\n",
      "[17/50] ğŸ‹ï¸ Train Loss: 0.0274 | ğŸ§ª Val Loss: 0.0241\n",
      "[18/50] ğŸ‹ï¸ Train Loss: 0.0275 | ğŸ§ª Val Loss: 0.0244\n",
      "[19/50] ğŸ‹ï¸ Train Loss: 0.0278 | ğŸ§ª Val Loss: 0.0257\n",
      "[20/50] ğŸ‹ï¸ Train Loss: 0.0272 | ğŸ§ª Val Loss: 0.0235\n",
      "[21/50] ğŸ‹ï¸ Train Loss: 0.0283 | ğŸ§ª Val Loss: 0.0232\n",
      "[22/50] ğŸ‹ï¸ Train Loss: 0.0269 | ğŸ§ª Val Loss: 0.0229\n",
      "[23/50] ğŸ‹ï¸ Train Loss: 0.0263 | ğŸ§ª Val Loss: 0.0227\n",
      "[24/50] ğŸ‹ï¸ Train Loss: 0.0292 | ğŸ§ª Val Loss: 0.0234\n",
      "[25/50] ğŸ‹ï¸ Train Loss: 0.0267 | ğŸ§ª Val Loss: 0.0235\n",
      "[26/50] ğŸ‹ï¸ Train Loss: 0.0269 | ğŸ§ª Val Loss: 0.0225\n",
      "[27/50] ğŸ‹ï¸ Train Loss: 0.0270 | ğŸ§ª Val Loss: 0.0225\n",
      "[28/50] ğŸ‹ï¸ Train Loss: 0.0281 | ğŸ§ª Val Loss: 0.0226\n",
      "[29/50] ğŸ‹ï¸ Train Loss: 0.0269 | ğŸ§ª Val Loss: 0.0231\n",
      "[30/50] ğŸ‹ï¸ Train Loss: 0.0258 | ğŸ§ª Val Loss: 0.0222\n",
      "[31/50] ğŸ‹ï¸ Train Loss: 0.0261 | ğŸ§ª Val Loss: 0.0222\n",
      "[32/50] ğŸ‹ï¸ Train Loss: 0.0278 | ğŸ§ª Val Loss: 0.0240\n",
      "[33/50] ğŸ‹ï¸ Train Loss: 0.0288 | ğŸ§ª Val Loss: 0.0369\n",
      "[34/50] ğŸ‹ï¸ Train Loss: 0.0273 | ğŸ§ª Val Loss: 0.0224\n",
      "[35/50] ğŸ‹ï¸ Train Loss: 0.0257 | ğŸ§ª Val Loss: 0.0224\n",
      "[36/50] ğŸ‹ï¸ Train Loss: 0.0263 | ğŸ§ª Val Loss: 0.0216\n",
      "[37/50] ğŸ‹ï¸ Train Loss: 0.0256 | ğŸ§ª Val Loss: 0.0237\n",
      "[38/50] ğŸ‹ï¸ Train Loss: 0.0256 | ğŸ§ª Val Loss: 0.0212\n",
      "[39/50] ğŸ‹ï¸ Train Loss: 0.0286 | ğŸ§ª Val Loss: 0.0209\n",
      "[40/50] ğŸ‹ï¸ Train Loss: 0.1199 | ğŸ§ª Val Loss: 0.0244\n",
      "[41/50] ğŸ‹ï¸ Train Loss: 0.0295 | ğŸ§ª Val Loss: 0.0245\n",
      "[42/50] ğŸ‹ï¸ Train Loss: 0.0286 | ğŸ§ª Val Loss: 0.0249\n",
      "[43/50] ğŸ‹ï¸ Train Loss: 0.0281 | ğŸ§ª Val Loss: 0.0247\n",
      "[44/50] ğŸ‹ï¸ Train Loss: 0.0291 | ğŸ§ª Val Loss: 0.0246\n",
      "[45/50] ğŸ‹ï¸ Train Loss: 0.0294 | ğŸ§ª Val Loss: 0.0237\n",
      "[46/50] ğŸ‹ï¸ Train Loss: 0.0271 | ğŸ§ª Val Loss: 0.0241\n",
      "[47/50] ğŸ‹ï¸ Train Loss: 0.0275 | ğŸ§ª Val Loss: 0.0247\n",
      "[48/50] ğŸ‹ï¸ Train Loss: 0.0274 | ğŸ§ª Val Loss: 0.0234\n",
      "[49/50] ğŸ‹ï¸ Train Loss: 0.0266 | ğŸ§ª Val Loss: 0.0268\n",
      "[50/50] ğŸ‹ï¸ Train Loss: 0.0264 | ğŸ§ª Val Loss: 0.0225\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 50\n",
    "#2. Training + Validation Loop\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "\n",
    "    for images, masks in train_loader:\n",
    "        images = images.to(device)\n",
    "        masks = masks.to(device)\n",
    "\n",
    "        preds = model(images)\n",
    "        loss = criterion(preds, masks)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item() * images.size(0)\n",
    "\n",
    "    avg_train_loss = train_loss / len(train_loader.dataset)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, masks in val_loader:\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device)\n",
    "\n",
    "            preds = model(images)\n",
    "            loss = criterion(preds, masks)\n",
    "            val_loss += loss.item() * images.size(0)\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader.dataset)\n",
    "\n",
    "    print(f\"[{epoch+1}/{num_epochs}] ğŸ‹ï¸ Train Loss: {avg_train_loss:.4f} | ğŸ§ª Val Loss: {avg_val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"unet_dagm_class1.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved at: /home/emre/Documents/GitHub/Digital-Image-Fault-Detection/src\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(\"Saved at:\", os.getcwd())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
